{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "8dbc2ce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dawid\\Desktop\\projekty\\wine_quality\n"
     ]
    }
   ],
   "source": [
    "#sciezka z danymi\n",
    "%cd \"C:\\Users\\Dawid\\Desktop\\projekty\\wine_quality\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "fc51dc38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#glowne biblioteki\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "from sklearn.metrics import *\n",
    "from sklearn.model_selection import  train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import tensorflow as tf\n",
    "from keras import layers\n",
    "from tensorflow.keras import layers, callbacks\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "#wlasny modul\n",
    "from dane.dane_wsadowe_zmiana_klas import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "15c9814d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformujemy Y_test i Y_train do wlasciwego formatu\n",
    "\n",
    "Y_train_enc = np.array(Y_train_all)\n",
    "Y_test_enc = np.array(Y_test)\n",
    "\n",
    "Y_test_enc = OneHotEncoder().fit_transform(Y_test_enc.reshape(-1,1)).toarray()\n",
    "Y_train_enc = OneHotEncoder().fit_transform(Y_train_enc.reshape(-1,1)).toarray()\n",
    "\n",
    "# zmienmy jeszcze format\n",
    "Y_test_enc = Y_test_enc.astype(np.float32)\n",
    "Y_train_enc = Y_train_enc.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "2775fdd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#definicja modelu\n",
    "def model_nn(X,Y, units, drop, classNum, epoch, lr = 0.01, bn = False, pat = 10):\n",
    "    '''\n",
    "    X - dane treningowe w postaci array bez labeli,\n",
    "    Y - labeli dla danych treningowych\n",
    "    units - lista unitow dla poszczegolnych warstw\n",
    "    drop - poziom dropoutu dla poszczegolnej wartswy\n",
    "    classNum = ilosc klas \n",
    "    epoch - liczba epok podczas trenowania \n",
    "    lr - learning rate\n",
    "    bn - czy zastosowac batch normalization \n",
    "    pat - ile interacji musi minac, aby przedwczesnie zatrzymac iterowanie\n",
    "    '''\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(layers.InputLayer(input_shape = X.shape[1]))\n",
    "    for i, j in enumerate(units):\n",
    "        model.add(layers.Dense(j, activation='relu', name=\"ukryta_\" + str(i)))\n",
    "        model.add(layers.Dropout(drop))\n",
    "        if bn == True:\n",
    "            model.add(layers.BatchNormalization())\n",
    "                  \n",
    "    model.add(layers.Dense(classNum, name=\"WYJSCIE\"))\n",
    "    model.add(layers.Softmax())\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    optymalizator = tf.optimizers.Adam(lr)\n",
    "    \n",
    "    model.compile(optimizer=optymalizator,\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "              metrics=['AUC'])\n",
    "    \n",
    "    results = model.fit(X,Y, validation_split= 0.2, epochs=epoch, verbose=1,\n",
    "              callbacks=[callbacks.EarlyStopping(monitor= 'val_auc', min_delta=0.1, \n",
    "                                                    patience=pat)])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e4ab9e32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "ukryta_0 (Dense)             (None, 256)               3072      \n",
      "_________________________________________________________________\n",
      "dropout_27 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "WYJSCIE (Dense)              (None, 2)                 514       \n",
      "_________________________________________________________________\n",
      "softmax_14 (Softmax)         (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 3,586\n",
      "Trainable params: 3,586\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "32/32 [==============================] - 1s 16ms/step - loss: 0.3419 - auc: 0.9355 - val_loss: 0.3057 - val_auc: 0.9388\n",
      "Epoch 2/30\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.2818 - auc: 0.9514 - val_loss: 0.2942 - val_auc: 0.9479\n",
      "Epoch 3/30\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.2688 - auc: 0.9567 - val_loss: 0.2916 - val_auc: 0.9488\n",
      "Epoch 4/30\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.2579 - auc: 0.9600 - val_loss: 0.2897 - val_auc: 0.9496\n",
      "Epoch 5/30\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.2208 - auc: 0.971 - 0s 4ms/step - loss: 0.2494 - auc: 0.9626 - val_loss: 0.2856 - val_auc: 0.9502\n",
      "Epoch 6/30\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.2455 - auc: 0.9635 - val_loss: 0.2980 - val_auc: 0.9470\n",
      "Epoch 7/30\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.2401 - auc: 0.9647 - val_loss: 0.3138 - val_auc: 0.9436\n",
      "Epoch 8/30\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.2344 - auc: 0.9663 - val_loss: 0.3055 - val_auc: 0.9457\n",
      "Epoch 9/30\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.2226 - auc: 0.9696 - val_loss: 0.2909 - val_auc: 0.9497\n",
      "Epoch 10/30\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.2242 - auc: 0.9693 - val_loss: 0.3214 - val_auc: 0.9418\n",
      "Epoch 11/30\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.2262 - auc: 0.9685 - val_loss: 0.3166 - val_auc: 0.9405\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.2776 - auc: 0.9548\n",
      "loss: 0.2776101231575012\n",
      "auc: 0.9548341035842896\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.95      0.93       280\n",
      "           1       0.52      0.40      0.45        40\n",
      "\n",
      "    accuracy                           0.88       320\n",
      "   macro avg       0.72      0.67      0.69       320\n",
      "weighted avg       0.87      0.88      0.87       320\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# wywolajmy pierwszy prosty model\n",
    "model_nn1 = model_nn(X_train_all_scaled, Y_train_enc,[256],0.1,2,30)\n",
    "Y_pred_nn1 = model_nn1.predict(X_test_scaled).argmax(axis = 1)\n",
    "\n",
    "# szybka ewaluacja\n",
    "eval_res = model_nn1.evaluate(X_test_scaled, Y_test_enc)\n",
    "eval_names = model_nn1.metrics_names\n",
    "print(f\"{eval_names[0]}: {eval_res[0]}\")\n",
    "print(f\"{eval_names[1]}: {eval_res[1]}\")\n",
    "\n",
    "#obejrzyjmy jeszcze macierz pomyłek \n",
    "matrix_nn1 = confusion_matrix(Y_test_enc.argmax(axis = 1), Y_pred_nn1)\n",
    "raport_nn1 = classification_report(Y_test_enc.argmax(axis = 1), Y_pred_nn1)\n",
    "print(raport_nn1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "49c2d659",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_26\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "ukryta_0 (Dense)             (None, 1024)              12288     \n",
      "_________________________________________________________________\n",
      "dropout_44 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "WYJSCIE (Dense)              (None, 2)                 2050      \n",
      "_________________________________________________________________\n",
      "softmax_26 (Softmax)         (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 14,338\n",
      "Trainable params: 14,338\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 0.3667 - auc: 0.9259 - val_loss: 0.3093 - val_auc: 0.9437\n",
      "Epoch 2/30\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.2816 - auc: 0.9531 - val_loss: 0.3214 - val_auc: 0.9370\n",
      "Epoch 3/30\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.2677 - auc: 0.9566 - val_loss: 0.3381 - val_auc: 0.9294\n",
      "Epoch 4/30\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.2714 - auc: 0.9557 - val_loss: 0.3020 - val_auc: 0.9469\n",
      "Epoch 5/30\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.2558 - auc: 0.9599 - val_loss: 0.3047 - val_auc: 0.9469\n",
      "Epoch 6/30\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.2501 - auc: 0.9617 - val_loss: 0.2975 - val_auc: 0.9453\n",
      "Epoch 7/30\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.2514 - auc: 0.9626 - val_loss: 0.3629 - val_auc: 0.9309\n",
      "Epoch 8/30\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.2444 - auc: 0.9641 - val_loss: 0.3359 - val_auc: 0.9433\n",
      "Epoch 9/30\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.2730 - auc: 0.9571 - val_loss: 0.3313 - val_auc: 0.9435\n",
      "Epoch 10/30\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.2327 - auc: 0.9674 - val_loss: 0.3267 - val_auc: 0.9346\n",
      "Epoch 11/30\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.2197 - auc: 0.9712 - val_loss: 0.3102 - val_auc: 0.9431\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.2822 - auc: 0.9539\n",
      "loss: 0.2822459936141968\n",
      "auc: 0.9539453387260437\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.94      0.93       280\n",
      "           1       0.51      0.47      0.49        40\n",
      "\n",
      "    accuracy                           0.88       320\n",
      "   macro avg       0.72      0.71      0.71       320\n",
      "weighted avg       0.87      0.88      0.88       320\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# po kilku probach nasz najlepszy model wyglada nastepujaco\n",
    "model_nn2 = model_nn(X_train_all_scaled, Y_train_enc,[1024],0.1,2,30)\n",
    "Y_pred_nn2 = model_nn2.predict(X_test_scaled).argmax(axis = 1)\n",
    "\n",
    "# szybka ewaluacja\n",
    "eval_res = model_nn2.evaluate(X_test_scaled, Y_test_enc)\n",
    "eval_names = model_nn2.metrics_names\n",
    "print(f\"{eval_names[0]}: {eval_res[0]}\")\n",
    "print(f\"{eval_names[1]}: {eval_res[1]}\")\n",
    "\n",
    "#obejrzyjmy jeszcze macierz pomyłek \n",
    "matrix_nn2 = confusion_matrix(Y_test_enc.argmax(axis = 1), Y_pred_nn2)\n",
    "raport_nn2 = classification_report(Y_test_enc.argmax(axis = 1), Y_pred_nn2)\n",
    "print(raport_nn2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84932946",
   "metadata": {},
   "source": [
    "Wyniki nie sa lepsze niz gdy korzystalismy z XGboosta, spróbujmy poradzic cos na nasze niezbalansowanie w zbiorze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4c7e6130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# skorzystajmy z dostepnej metody z dedykowanej biblioteki\n",
    "smote = SMOTE(sampling_strategy = 'minority',random_state=1)\n",
    "x_sm, y_sm = smote.fit_resample(X,Y)\n",
    "\n",
    "\n",
    "# podzielmy jeszcze raz zbior na zbior treningowy i testowy\n",
    "X_train1, X_test1, Y_train1,Y_test1 = train_test_split(x_sm,y_sm, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d86cf1bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    1120\n",
       "2    1091\n",
       "Name: quality_category, dtype: int64"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#jak pamietamy nasz data set był niezbalansowany sprawdzimy czy oversampling poprawił nam udziały klasy.\n",
    "Y_train1.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f5826b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Przetworzmy dane do formatu wejsciowego do modelu\n",
    "\n",
    "X_train_array_d = X_train1.values\n",
    "X_test_array_d = X_test1.values\n",
    "\n",
    "\n",
    "# transformujemy Y_test_duplicated i Y_test_duplicated\n",
    "\n",
    "Y_train_d = np.array(Y_train1)\n",
    "Y_test_d = np.array(Y_test1)\n",
    "\n",
    "Y_test_d = OneHotEncoder().fit_transform(Y_test_d.reshape(-1,1)).toarray()\n",
    "# zmienmy jeszcze format\n",
    "Y_test_d = Y_test_d.astype(np.float32)\n",
    "\n",
    "# transformujemy y_train\n",
    "Y_train_d = OneHotEncoder().fit_transform(Y_train_d.reshape(-1,1)).toarray()\n",
    "# zmienmy jeszcze format\n",
    "Y_train_d = Y_train_d.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "da15e1f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_45\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "ukryta_0 (Dense)             (None, 128)               1536      \n",
      "_________________________________________________________________\n",
      "dropout_76 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "WYJSCIE (Dense)              (None, 2)                 258       \n",
      "_________________________________________________________________\n",
      "softmax_45 (Softmax)         (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 1,794\n",
      "Trainable params: 1,794\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "56/56 [==============================] - 2s 12ms/step - loss: 1.1632 - auc: 0.6060 - val_loss: 0.6232 - val_auc: 0.7271\n",
      "Epoch 2/30\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.6491 - auc: 0.7209 - val_loss: 0.6454 - val_auc: 0.7430\n",
      "Epoch 3/30\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.5939 - auc: 0.7560 - val_loss: 0.5207 - val_auc: 0.8371\n",
      "Epoch 4/30\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.5592 - auc: 0.7885 - val_loss: 0.5053 - val_auc: 0.8458\n",
      "Epoch 5/30\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.5073 - auc: 0.8322 - val_loss: 0.4960 - val_auc: 0.8497\n",
      "Epoch 6/30\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.5107 - auc: 0.8294 - val_loss: 0.4915 - val_auc: 0.8442\n",
      "Epoch 7/30\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.4657 - auc: 0.8609 - val_loss: 0.4676 - val_auc: 0.8573\n",
      "Epoch 8/30\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.4667 - auc: 0.8588 - val_loss: 0.4933 - val_auc: 0.8440\n",
      "Epoch 9/30\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.4641 - auc: 0.8627 - val_loss: 0.5113 - val_auc: 0.8260\n",
      "Epoch 10/30\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.4741 - auc: 0.8565 - val_loss: 0.6314 - val_auc: 0.7750\n",
      "Epoch 11/30\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.4789 - auc: 0.8539 - val_loss: 0.4601 - val_auc: 0.8647\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4554 - auc: 0.8670\n",
      "loss: 0.4553665220737457\n",
      "auc: 0.8670461177825928\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.68      0.76       262\n",
      "           1       0.76      0.90      0.82       291\n",
      "\n",
      "    accuracy                           0.80       553\n",
      "   macro avg       0.81      0.79      0.79       553\n",
      "weighted avg       0.81      0.80      0.79       553\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#sprawdzmy jak z przerobionym zestawem danych dziala nasza siec neuronowa\n",
    "model_nn3 = model_nn(X_train_array_d, Y_train_d,[128],0.1,2,30)\n",
    "Y_pred_nn3 = model_nn3.predict(X_test_array_d).argmax(axis = 1)\n",
    "\n",
    "# szybka ewaluacja\n",
    "eval_res = model_nn3.evaluate(X_test_array_d, Y_test_d)\n",
    "eval_names = model_nn3.metrics_names\n",
    "print(f\"{eval_names[0]}: {eval_res[0]}\")\n",
    "print(f\"{eval_names[1]}: {eval_res[1]}\")\n",
    "\n",
    "#obejrzyjmy jeszcze macierz pomyłek \n",
    "matrix_nn3 = confusion_matrix(Y_test_d.argmax(axis = 1), Y_pred_nn3)\n",
    "raport_nn3 = classification_report(Y_test_d.argmax(axis = 1), Y_pred_nn3)\n",
    "print(raport_nn3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efba3eac",
   "metadata": {},
   "source": [
    "Nasz model znacznie poprawil rezultaty dla drugiej klasy, ale przez to spadla dokladnosc dla klasy, ktora pierwotnie byla mniej liczna. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "5ada4596",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_89\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "ukryta_0 (Dense)             (None, 256)               3072      \n",
      "_________________________________________________________________\n",
      "dropout_132 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "WYJSCIE (Dense)              (None, 2)                 514       \n",
      "_________________________________________________________________\n",
      "softmax_89 (Softmax)         (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 3,586\n",
      "Trainable params: 3,586\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "56/56 [==============================] - 1s 9ms/step - loss: 1.5363 - auc: 0.6183 - val_loss: 0.5947 - val_auc: 0.7553\n",
      "Epoch 2/30\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.5828 - auc: 0.7665 - val_loss: 0.6899 - val_auc: 0.7002\n",
      "Epoch 3/30\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.5453 - auc: 0.8056 - val_loss: 0.5933 - val_auc: 0.7521\n",
      "Epoch 4/30\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.5333 - auc: 0.8129 - val_loss: 0.4747 - val_auc: 0.8499\n",
      "Epoch 5/30\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.4952 - auc: 0.8433 - val_loss: 0.4616 - val_auc: 0.8589\n",
      "Epoch 6/30\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.4598 - auc: 0.8636 - val_loss: 0.4553 - val_auc: 0.8699\n",
      "Epoch 7/30\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.4545 - auc: 0.8678 - val_loss: 0.4834 - val_auc: 0.8529\n",
      "Epoch 8/30\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.4672 - auc: 0.8611 - val_loss: 0.4351 - val_auc: 0.8789\n",
      "Epoch 9/30\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.4143 - auc: 0.8915 - val_loss: 0.4369 - val_auc: 0.8757\n",
      "Epoch 10/30\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.4960 - auc: 0.8449 - val_loss: 0.4965 - val_auc: 0.8469\n",
      "Epoch 11/30\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.4340 - auc: 0.8798 - val_loss: 0.4195 - val_auc: 0.8863\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4332 - auc: 0.8815\n",
      "loss: 0.43320298194885254\n",
      "auc: 0.8814783692359924\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.73      0.78       262\n",
      "           1       0.78      0.88      0.83       291\n",
      "\n",
      "    accuracy                           0.81       553\n",
      "   macro avg       0.81      0.80      0.81       553\n",
      "weighted avg       0.81      0.81      0.81       553\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#sprobujmy jeszcze inne konfiguracje\n",
    "model_nn4 = model_nn(X_train_array_d, Y_train_d,[256],0.0,2,30)\n",
    "Y_pred_nn4 = model_nn4.predict(X_test_array_d).argmax(axis = 1)\n",
    "\n",
    "# szybka ewaluacja\n",
    "eval_res = model_nn4.evaluate(X_test_array_d, Y_test_d)\n",
    "eval_names = model_nn4.metrics_names\n",
    "print(f\"{eval_names[0]}: {eval_res[0]}\")\n",
    "print(f\"{eval_names[1]}: {eval_res[1]}\")\n",
    "\n",
    "#obejrzyjmy jeszcze macierz pomyłek \n",
    "matrix_nn4 = confusion_matrix(Y_test_d.argmax(axis = 1), Y_pred_nn4)\n",
    "raport_nn4 = classification_report(Y_test_d.argmax(axis = 1), Y_pred_nn4)\n",
    "print(raport_nn4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1b972f",
   "metadata": {},
   "source": [
    "Nasz najlepszy model uzyskał całkiem dobry wynik dla obu klas, zapiszmy jego wynik."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "5ea5e81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nn4.save(\"modele/model_nn4.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
